[
  {
    "path": "posts/2021-02-04-curso-r-regresso-linear-br-exerccios-rlsm/",
    "title": "Exercícios RLSM",
    "description": "Resolução da lista de exercícios do Curso de Regressão Linear com R de janeiro de 2021.",
    "author": [
      {
        "name": "Elizabeth Mie Hashimoto",
        "url": "https://www.linkedin.com/in/elizabeth-mie-hashimoto-a416a917/"
      }
    ],
    "date": "2021-01-25",
    "categories": [],
    "contents": "\nPacotes\n\n\nlibrary(tidyverse) # manipulacao de data.frame\nlibrary(MASS) # dados Boston\nlibrary(broom)\n\n\n\nDados\nO banco de dados Boston apresenta registros de valores medianos das casas (medv) de 506 bairros de Boston. O objetivo é identificar quais das 13 variáveis explicativas estão associadas com esses valores e usá-las para fazer predições de preços das casas.\n\n\nglimpse(Boston)\n\n\nRows: 506\nColumns: 14\n$ crim    <dbl> 0.00632, 0.02731, 0.02729, 0.03237, 0.06905, 0.0298…\n$ zn      <dbl> 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.5, 12.5, 12.5, 12…\n$ indus   <dbl> 2.31, 7.07, 7.07, 2.18, 2.18, 2.18, 7.87, 7.87, 7.8…\n$ chas    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ nox     <dbl> 0.538, 0.469, 0.469, 0.458, 0.458, 0.458, 0.524, 0.…\n$ rm      <dbl> 6.575, 6.421, 7.185, 6.998, 7.147, 6.430, 6.012, 6.…\n$ age     <dbl> 65.2, 78.9, 61.1, 45.8, 54.2, 58.7, 66.6, 96.1, 100…\n$ dis     <dbl> 4.0900, 4.9671, 4.9671, 6.0622, 6.0622, 6.0622, 5.5…\n$ rad     <int> 1, 2, 2, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, …\n$ tax     <dbl> 296, 242, 242, 222, 222, 222, 311, 311, 311, 311, 3…\n$ ptratio <dbl> 15.3, 17.8, 17.8, 18.7, 18.7, 18.7, 15.2, 15.2, 15.…\n$ black   <dbl> 396.90, 396.90, 392.83, 394.63, 396.90, 394.12, 395…\n$ lstat   <dbl> 4.98, 9.14, 4.03, 2.94, 5.33, 5.21, 12.43, 19.15, 2…\n$ medv    <dbl> 24.0, 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.…\n\n\n\n# Descrição das variáveis\nhelp(Boston)\n\n\n\nExercício 1\nFaça um gráfico de dispersão entre medv e rm.\n\n\nBoston %>% \n  ggplot() +\n  geom_point(aes(x = rm, y = medv)) +\n  labs(x = 'Número médio de quartos por habitação', y = 'Preço mediano das habitações do bairro (em 1000 dólares)')\n\n\n\n\nExercício 2\nAjuste um modelo de regressão linear simples utilizando medv como resposta e rm como explicativa e guarde em objeto chamado mod_simples. Consulte o summary(mod_simples) em seguida.\n\n\nmod_simples <- lm(medv ~ rm, data=Boston)\nsummary(mod_simples)\n\n\n\nCall:\nlm(formula = medv ~ rm, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-23.346  -2.547   0.090   2.986  39.433 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -34.671      2.650  -13.08   <2e-16 ***\nrm             9.102      0.419   21.72   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.616 on 504 degrees of freedom\nMultiple R-squared:  0.4835,    Adjusted R-squared:  0.4825 \nF-statistic: 471.8 on 1 and 504 DF,  p-value: < 2.2e-16\n\nExercício 3\nSabendo que medv é o preço mediano das habitações do bairro e o rm é o número médio de quartos por habitação,\ninterprete o parâmetro (Intercept).\n\\(\\blacktriangleright\\) Resposta: Nesse caso, -34.671 \\(\\times\\) 1000 dólares é a média do preço mediano das habitações quando temos zero quartos por habitação.\ninterprete o parâmetro rm.\n\\(\\blacktriangleright\\) Resposta: A cada um quarto por habitação que aumentamos por habitação, temos um acréscimo de 9.102 \\(\\times\\) 1000 dólares na média do preço mediano das habitações.\no número de quartos está associado com o valor da habitação? Por quê?\n\\(\\blacktriangleright\\) Resposta: Sim, pois ao nível de significância de 5%, rejeitamos a hipótese nula (\\(H_0: \\beta_1=0\\)), uma vez que o \\(p\\)-valor é menor do que 0,001. Dessa forma, temos evidências de que o preço mediano das habitações tem alguma relação com o número médio de quartos por habitação.\nExercício 4\nConsulte as saídas das funções\ncoef(mod_simples): mostra apenas as estimativas dos coeficientes de regressão.\nconfint(mod_simples): mostra o intervalo de 95% de confiança das estimativas dos coeficientes de regressão.\npredict(mod_simples): calcula os valores preditos do preço mediano das habitações, isto é, \\(\\hat{\\mbox{medv}}=-34,671+9,102 rm\\).\npredict(mod_simples, interval = \"confidence\"): calcula os valores preditos do preço mediano das habitações e o intervalo de 95% de confiança de cada valor predito.\naugment(mod_simples): cria um data frame com valores de medv, rm, valores preditos, resíduo e distância de Cook.\n\n\ncoef(mod_simples)\n\n\n(Intercept)          rm \n -34.670621    9.102109 \n\nconfint(mod_simples)\n\n\n                 2.5 %     97.5 %\n(Intercept) -39.876641 -29.464601\nrm            8.278855   9.925363\n\npredict(mod_simples) %>% head(n=10L)\n\n\n       1        2        3        4        5        6        7 \n25.17575 23.77402 30.72803 29.02594 30.38215 23.85594 20.05126 \n       8        9       10 \n21.50760 16.58335 19.97844 \n\npredict(mod_simples, interval = \"confidence\") %>% head(n=10L)\n\n\n        fit      lwr      upr\n1  25.17575 24.55039 25.80110\n2  23.77402 23.18536 24.36269\n3  30.72803 29.78817 31.66790\n4  29.02594 28.20203 29.84984\n5  30.38215 29.46676 31.29755\n6  23.85594 23.26582 24.44606\n7  20.05126 19.43134 20.67118\n8  21.50760 20.92234 22.09285\n9  16.58335 15.79375 17.37296\n10 19.97844 19.35611 20.60078\n\naugment(mod_simples)\n\n\n# A tibble: 506 x 8\n    medv    rm .fitted  .resid .std.resid    .hat .sigma     .cooksd\n   <dbl> <dbl>   <dbl>   <dbl>      <dbl>   <dbl>  <dbl>       <dbl>\n 1  24    6.58    25.2 -1.18      -0.178  0.00231   6.62 0.0000367  \n 2  21.6  6.42    23.8 -2.17      -0.329  0.00205   6.62 0.000111   \n 3  34.7  7.18    30.7  3.97       0.602  0.00523   6.62 0.000952   \n 4  33.4  7.00    29.0  4.37       0.662  0.00402   6.62 0.000885   \n 5  36.2  7.15    30.4  5.82       0.882  0.00496   6.62 0.00194    \n 6  28.7  6.43    23.9  4.84       0.733  0.00206   6.62 0.000555   \n 7  22.9  6.01    20.1  2.85       0.431  0.00227   6.62 0.000212   \n 8  27.1  6.17    21.5  5.59       0.846  0.00203   6.62 0.000727   \n 9  16.5  5.63    16.6 -0.0834    -0.0126 0.00369   6.62 0.000000295\n10  18.9  6.00    20.0 -1.08      -0.163  0.00229   6.62 0.0000306  \n# … with 496 more rows\n\nExercício 5\nUsando o data.frame gerado por augment(mod_simples) faça um gráfico de medv versus rm e em seguida desenhe a reta ajustada do mod_simples.\n\n\nboston_pred <- augment(mod_simples)\n\nboston_pred %>% \n  ggplot() +\n  geom_point(aes(x = rm, y = medv)) +\n  geom_line(aes(x = rm, y = .fitted), color=\"red\") +\n  labs(x = 'Número médio de quartos por habitação', y = 'Preço mediano das habitações do bairro (em 1000 dólares)')\n\n\n\n\nExercício 6\nFaça um gráfico de resíduos. Coloque os resíduos no eixo Y e os valores ajustados no eixo X.\n\n\nboston_pred %>% \n  ggplot() +\n  geom_point(aes(x = .fitted, y = .std.resid)) +\n  geom_hline(yintercept=0, linetype=\"dashed\") +\n  labs(x = 'Valores ajustados', y = 'Resíduos')\n\n\n\n\nExercício 7\nObserve os gráficos de plot(mod_simples).\n\n\nplot(mod_simples)\n\n\n\n\nApenas pela inspeção visual, responda: existem outliers? Eles são pontos de alavanca?\n\\(\\blacktriangleright\\) Resposta: Por meio das figuras, observamos que os pontos \\(\\sharp366\\), \\(\\sharp369\\) e \\(\\sharp373\\) são possíveis outliers. O gráfico de Residuals vs Leverage indica que não são pontos de alavanca.\nExercício 8\nAjuste um modelo mod_multiplo para medv explicado por rm e crim. Consulte o summary(mod_multiplo) em seguida.\n\n\nmod_multiplo <- lm(medv ~ rm + crim, data=Boston)\nsummary(mod_multiplo)\n\n\n\nCall:\nlm(formula = medv ~ rm + crim, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-21.608  -2.835  -0.380   2.592  38.839 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -29.24472    2.58809 -11.300   <2e-16 ***\nrm            8.39107    0.40485  20.726   <2e-16 ***\ncrim         -0.26491    0.03307  -8.011    8e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.237 on 503 degrees of freedom\nMultiple R-squared:  0.542, Adjusted R-squared:  0.5401 \nF-statistic: 297.6 on 2 and 503 DF,  p-value: < 2.2e-16\n\nExercício 9\nQual modelo ficou melhor: mod_simples ou mod_multiplo? Qual critério você utilizou para decidir o melhor?\n\\(\\blacktriangleright\\) Resposta: O mod_multiplo parece ser melhor do que o mod_simples, porque, considerando o R2 ajustado, o R2 ajustado do mod_multiplo (0,5401) é maior do que o R2 ajustado do mod_simples (0,4825). Além disso, a variável crim é significativa (\\(p\\)-valor < 0,001) para explicar a variablidade presente na média do preço mediano das habitações, considerando um nível de significância de 5%.\nPor outro lado, pelos resíduos, ambos os modelos tem problemas com em relação a normalidade dos resíduos e com uma possível relação não linear entre a variável resposta e as variáveis explicativas. O que indica que o modelo mod_multiplo pode ser melhorado.\n\n\n# Resíduo modelo múltiplo\nplot(mod_multiplo)\n\n\n\n\n\n\n\nExercício 10\nAjuste um modelo mod_completo para medv explicado por todas as demais colunas. DICA: na fórmula medv ~ ., o ponto significa “todas as variáveis, tirando medv”.\nConsulte o summary(mod_completo) em seguida.\n\n\nmod_completo <- lm(medv ~ ., data=Boston)\nsummary(mod_completo)\n\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  < 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: < 2.2e-16\n\nQual modelo ficou melhor: mod_simples, mod_multiplo ou mod_completo?\n\\(\\blacktriangleright\\) Resposta: Novamente considerando o R2 ajustado, o modelo mais adequado entre os três modelos é o mod_completo, pois tem o R2 ajustado é igual a 0,7338; que é maior do que o R2 ajustados dos demais modelos. Além disso, as variáveis explicativas, exceto indus e age, foram signifitivas ao nível de significância de 5%.\nEm relação aos resíduos, o modelo completo tem o mesmo problema dos outros dois modelos. Nesse caso, o modelo completo também pode ser melhorado.\n\n\n# Resíduo modelo completo\nplot(mod_completo)\n\n\n\n\nO valor estimado para o termo rm variou entre os três modelos? Por qual razão você acha que isso aconteceu?\n\\(\\blacktriangleright\\) Resposta: Sim, a estimativa para o termo rm variou devido a inclusão de variáveis de explicativas no modelo.\n\n\n\n",
    "preview": "posts/2021-02-04-curso-r-regresso-linear-br-exerccios-rlsm/curso-r-regresso-linear-br-exerccios-rlsm_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-02-05T15:56:13-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-05-tcc/",
    "title": "TCC",
    "description": "Resolução do TCC do curso de Regressão Linear com R de janeiro de 2021.",
    "author": [
      {
        "name": "Ricardo Feliz Okamoto",
        "url": "https://www.linkedin.com/in/ricardo-feliz-okamoto-a20344171/"
      }
    ],
    "date": "2021-01-25",
    "categories": [],
    "contents": "\nIntrodução\nA eficiência dos carros é uma das principais características que a indústria automobilística tenta optimizar. Melhorar a eficiência de um carro significa aumentar a quantidade de quilômetros que os carros conseguem rodar consumindo menos combustível. Não só para a indústria automobilística esse tema tem importância, como também para qualquer cidadão que esteja preocupado em gastar menos (afinal de contas, mais quilômetros rodados com menos combustível significa uma conta menor no fim do mês de gasolina), e também para qualquer pessoa com consciência ambiental, que deseja diminuir a quantidade de combustíveis fósseis que são queimados por dia.\nDefinição do problema\nDada a relevância de se estudar este problema, é que eu me guio pela indagção de quais características do carro explicam sua a eficiência (milhas por galão de combustível)?\nDescrição básica da origem e da coleta dos dados\nPara estudar isso, utilizarei a base de dados mtcars. Essa base inclui 32 linhas e 11 colunas. Cada observação é um modelo de carro diferente; e cada coluna é uma característica desse carro. Ao todo, portanto, temos 11 características de 32 modelos de carro para comparar.\nA eficiência está representada nesta base de dados pela coluna mpg, que é a quantidade de milhas rodadas por galão de combustível (miles per galon).\nHipótese do estudo\nMinha hipótese é a de que as seguintes variáveis são relevantes para se explicar a eficiciência dos carros:\ncyl;\nhp;\nwt;\nvs;\nam;\ngear;\ncarb.\nSumários das variáveis (univariadas)\nVariável dependente\n\nShow code\nggplot(mtcars, aes(mpg)) +\n  geom_histogram(fill = \"steelblue\") +\n  geom_vline(aes(xintercept=mean(mpg)),\n            color=\"blue\", linetype=\"dashed\", size=1)\n\n\n\n\nA começar pela variável de interesse, mpg, observamos a seguinte distribuição dos modelos de carro por eficiência. A média de eficiência é de aproximadamente 20 mpg. Temos 18 modelos de carro abaixo da média e 14 modelos acima dela, sendo que o modelo com a pior eficiência roda apenas 10.40 milhas por galão, enquanto o modelo mais eficiente roda mais que o triplo disso, com 33.90 milhas por galão.\n\nShow code\nsummary(mtcars$mpg)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  10.40   15.43   19.20   20.09   22.80   33.90 \n\nVariáveis explicativas\nQuanto às variáveis explicativas, temos os seguintes gráficos.\n\nShow code\nggplot(mtcars, aes(cyl)) +\n  geom_histogram(fill = \"steelblue\") \n\n\n\n\nOs cilindros são onde o combustível é queimado. Há apenas três tipos de carros e cilindros: carros com 4 cilindros, 6 cilindros ou 8 cilindros. O que vemos é que a maioria dos carros possui 8 cilindros, seguido de carros com 4 cilindros e, em terceiro lugar, carros com 6 cilindros. Essa característica se relaciona com a eficiência porque quanto mais cilindros existem, maior é a queima de combustível e, portanto, maior é a geração de poder para o carro.\n\nShow code\nggplot(mtcars, aes(hp)) +\n  geom_histogram(fill = \"steelblue\") +\n  geom_vline(aes(xintercept=mean(hp)),\n            color=\"blue\", linetype=\"dashed\", size=1) +\n    geom_vline(aes(xintercept=median(hp)),\n            color=\"red\", linetype=\"dashed\", size=1)\n\n\n\n\nQuanto ao gross horsepower (hp), essa característica diz respeito aos cavalos do carro. De forma genérica, o horsepower é uma indicação do quanto que o veículo consegue se mover sozinho. Quanto mais cavalos, mais rápido o carro consegue ser. Essa característica se relaciona com a eficiência porque carros com menos horsepower necessitam de mais energia para se movimentarem e, do contrário, carros com mais horsepower necessitam de menos energia para se movimentarem. Dito de outra maneira, quanto maior o horsepower, maior é a eficicência, porque menor é o gasto energético.\nVemos que a distribuição do horsepower é assimétrica para a direita (right skewed) ao compararmos a média (azul) com a mediana (vermelho).\n\nShow code\nggplot(mtcars, aes(wt)) +\n  geom_histogram(fill = \"steelblue\") +\n  geom_vline(aes(xintercept=mean(wt)),\n            color=\"blue\", linetype=\"dashed\", size=1) +\n    geom_vline(aes(xintercept=median(wt)),\n            color=\"red\", linetype=\"dashed\", size=1) +\n  xlab(\"wt \\n (1000 lbs)\")\n\n\n\n\nOutra variável de interesse é o peso do carro. Quanto mais pesado é um caro, espera-se que menos eficiente ele seja.\nO que vemos desse dado é uma grande concentração de valores que giram entorno da média de aproxidamente 3200 libras.\n\nShow code\nggplot(mtcars, aes(vs)) +\n  geom_histogram(fill = \"steelblue\") +\n  scale_x_continuous(name = \"Engine \\n (0 = V-shaped, 1 = straight)\", breaks = c(0,1))\n\n\n\n\nA seguir, temos o tipo de motor. Essa variável é binária, ela assume apenas dois valores: ou o motoro é V-shaped, ou ele é straight. Na base de dados, essa característica está representada na coluna vs. Esperamos que o tipo de motor influencie na eficiência, porque é justamente no motor em que o consumo de combustível ocorre.\n\nShow code\nggplot(mtcars, aes(am)) +\n  geom_histogram(fill = \"steelblue\") +\n  scale_x_continuous(name = \"Transmission \\n (0 = Automatic, 1 = Manual)\", breaks = c(0,1)) \n\n\n\n\nOutra característica que esperamos que se relacione com a eficiência é o tipo de transmissão, se é manual (1) ou automático (0). Vemos que existem nessa base de dados mais carros automáticos do que manuais.\n\nShow code\nggplot(mtcars, aes(gear)) +\n  geom_histogram(fill = \"steelblue\") +\n  scale_x_continuous(breaks = c(3,4,5)) \n\n\n\n\nAlém disso, temos a quantidade de marchas que um carro possui. Existem carros apenas de 3, 4 ou 5 marchas. Pensamos que a marcha se relaciona com a eficiciência porque ela controla a quantidade de poder disponível do motor.\n\nShow code\nggplot(mtcars, aes(carb)) +\n  geom_histogram(fill = \"steelblue\") +\n  scale_x_continuous(breaks=c(1:8))\n\n\n\n\nPor fim, existe a quantidade de carburadores por carro. Os carburadores são o dispositivo dos carros que misturam ar e combustível para combustões internas. Espera-se que essa variável também se relacione com a eficiência do carro.\nGráficos de duas variáveis\n\nShow code\np_cyl <- ggplot(mtcars, aes(cyl, mpg)) +\n  geom_point()\n\np_hp <- ggplot(mtcars, aes(hp, mpg)) +\n  geom_point()\n\np_vs <- ggplot(mtcars, aes(vs, mpg)) +\n  geom_point()\n\np_am <- ggplot(mtcars, aes(am, mpg)) +\n  geom_point()\n\np_gear <- ggplot(mtcars, aes(gear, mpg)) +\n  geom_point()\n\np_carb <- ggplot(mtcars, aes(carb, mpg)) +\n  geom_point()\n\ng1 <- gridExtra::grid.arrange(\n  p_cyl, \n  p_hp, \n  nrow = 2\n)\n\n\nShow code\ng2 <- gridExtra::grid.arrange(\n  p_vs, \n  p_am, \n  nrow = 2\n)\n\n\nShow code\ng3 <- gridExtra::grid.arrange(\n  p_gear, \n  p_carb,\n  nrow = 2\n)\n\n\n\n\nModelagem\nBaseline model\nA fórmula básica, da qual partiremos, é a seguinte:\n\\[\n\\operatorname{baseline\\ model:\\ mpg} = \\beta_{0} + \\beta_{1}(\\operatorname{cyl}) + \\beta_{2}(\\operatorname{hp}) + + \\beta_{3}(\\operatorname{wt}) + \\beta_{4}(\\operatorname{vs}) + \\beta_{5}(\\operatorname{am}) + \\beta_{6}(\\operatorname{gear}) + \\beta_{7}(\\operatorname{carb}) + \\epsilon\n\\] Essa equação de regressão ainda não representa a especificação final do modelo. É possível que algumas dessas variáveis não possuem efeito estatisticamente relevante sobre a variável de interesse mpg. Ou ainda pode ser que haja multicolinearidade entre algumas variáveis, ou algum tipo de interação, ou relação polinomial. Então esta não é a versão ajustada do modelo, mas é o modelo inicial, o qual chamarei de “baseline model”.\nQuando rodamos a regressão, a nossa fórmula fica especificada com os seguintes coeficientes:\n\\[\n\\operatorname{baseline\\ model:\\ mpg} = 30.25 - 0.38(\\operatorname{cyl}) - 0.02(\\operatorname{hp}) - 2.18(\\operatorname{wt}) + 97(\\operatorname{vs}) + 2.11(\\operatorname{am}) + 0.66(\\operatorname{gear}) - 0.62(\\operatorname{carb}) + \\epsilon\n\\] Esses parâmetros ficam melhor resumidos na tabela a seguir:\n\nShow code\nbaseline <- lm(mpg~cyl + hp + wt + vs + am + gear + carb, mtcars)\nsummary(baseline)\n\n\n\nCall:\nlm(formula = mpg ~ cyl + hp + wt + vs + am + gear + carb, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.2242 -1.4538 -0.4293  1.4548  5.2956 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept) 30.25034    8.61471   3.511  0.00179 **\ncyl         -0.38285    0.87852  -0.436  0.66688   \nhp          -0.01859    0.01710  -1.088  0.28762   \nwt          -2.18465    1.02244  -2.137  0.04302 * \nvs           0.96917    1.85506   0.522  0.60615   \nam           2.11755    1.88907   1.121  0.27340   \ngear         0.65975    1.43338   0.460  0.64946   \ncarb        -0.62289    0.58164  -1.071  0.29485   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.578 on 24 degrees of freedom\nMultiple R-squared:  0.8583,    Adjusted R-squared:  0.817 \nF-statistic: 20.77 on 7 and 24 DF,  p-value: 9.689e-09\n\nOlhando para esse teste, conseguimos ver que apenas dois coeficientes possuem signficância estatística: o intercepto (B0) e o coeficiente relativo ao peso do carro (B2). O teste de hipótese relativo ao B0 indica que com 99.99% de certeza conseguimos rejeitar a hipótese nula de que o intercepto é igual a 0. Já o teste de hipótese relativo ao B2 indica que com 95% de certza conseguimos rejeitar a hipótese nula de que a relação entre aquele coeficiente com a variável de interesse (mpg) é 0. Todos os demais coeficientes não conseguem ter a hipótese nula rejeitada, isto é, não é possível dizer que a relação dessas variáveis explicativas com a variável de resposta seja diferente de 0.\nA partir disso, poderíamos ser levados a concluir que a única variável que explica a eficiência do motor é o peso do carro. Mas temos de tomar cuidado com essa conclusão. Ela é muito precipitada. O que pode estar acontecendo é que pode haver multicolinearidade ou interação entre as variáveis. Nesses casos, o efeito de uma variável sobre Y estaria sendo “roubado” e, por isso, ele fica invisível e não conseguimos rejeitar a hipótese nula de que o coeficiente é diferente de 0.\nMulticolinearidade e Interações\nAnálise de cyl e hp\nPara tentar desmistificar isso, precisamos avaliar as variáveis individualmente. Quando rodamos um modelo para cada variável, a história é outra. Todos os coeficientes se tornam estatisticamente significantes.\n\nShow code\ncyl_model <- lm(mpg~cyl, mtcars)\nhp_model <- lm(mpg~hp, mtcars)\nwt_model <- lm(mpg~wt, mtcars)\nvs_model <- lm(mpg~vs, mtcars)\nam_model <- lm(mpg~am, mtcars)\ngear_model <- lm(mpg~gear, mtcars)\ncarb_model <- lm(mpg~carb, mtcars)\n\njtools::export_summs(cyl_model, hp_model, wt_model, vs_model, am_model, gear_model, carb_model, scale=TRUE)\n\n\n─────────────────────────────────────────────────────────────────── Model 1 Model 2 Model 3 Model 4 Model 5\n──────────────────────────────────────────────────────── (Interce 20.09 20.09 20.09 16.62 17.15\npt) *** *** *** *** ***\n(0.57)   (0.68)   (0.54)   (1.08)   (1.12)  \n         \ncyl -5.14                                \n***\n(0.58)                                  \n \nhp         -4.68                        \n***\n        (0.69)                          \n \nwt                 -5.23                \n***\n                (0.55)                  \n \nvs                         7.94 ***        \n                        (1.63)          \n \nam                                 7.24 ***\n                                (1.76)  \n \ngear                                        \n                                       \ncarb                                        \n                                       \n──────────────────────────────────────────────────────── N 32       32       32       32       32      \n         \nR2 0.73     0.60     0.75     0.44     0.36    \n─────────────────────────────────────────────────────────────────── All continuous predictors are mean-centered and\nscaled by 1 standard deviation. *** p < 0.001;\n** p < 0.01; * p < 0.05.\nColumn names: names, Model 1, Model 2, Model 3, Model 4, Model 5, Model 6, Model 7\n6/8 columns shown.\n\nComparando, então, esses resultados com a equação que inclui todas as variáveis, podemos levantar fortes suspeitas de que existe multicolinearidade no nosso baseline model. Para diagnosticar a multicolinearidade, podemos usar o VIF.\n\nShow code\ncar::vif(baseline)\n\n\n      cyl        hp        wt        vs        am      gear      carb \n11.480450  6.409021  4.667601  4.076992  4.143941  5.216024  4.116132 \n\nO VIF nos dá alguns valores a que temos de nos atentar. Primeiro, observamos que o VIF do cyl é o maior de todos. Ele está acima de 5. Acima de 5 todos VIF é problemático. Outra variável problemática é hp. Além disso, há gear. Entretanto, como ele está apenas um pouco acima de 5, vou deixá-lo para uma análise posterior. O que eu vou testar agora é o que acontece com as demais variáveis ao retirar o cyl e hp.\n\nShow code\nbaseline_sem_cyl_e_hp <- lm(mpg~wt + vs + am + gear + carb, mtcars)\ncar::vif(baseline_sem_cyl_e_hp)\n\n\n      wt       vs       am     gear     carb \n4.395138 2.090715 3.881481 4.510498 3.160047 \n\nDe fato, o VIF das variáveis diminuem. Agora não temos nenhum VIF que passe de 5. Entretanto, acho que seria cauteloso nos determos melhor em wt, gear e am, pois seus valores são os mais próximos de 5.\nComecemos pela análise de wt, rodando separadamente um modelo com wt e apenas uma outra variável.\nAnálise de wt\n\nShow code\nwt1 <- lm(mpg~wt+vs, mtcars)\nwt2 <- lm(mpg~wt+am, mtcars)\nwt3 <- lm(mpg~wt+gear, mtcars)\nwt4 <- lm(mpg~wt+carb, mtcars)\n\njtools::export_summs(wt_model, wt1, wt2, wt3, wt4, scale=TRUE)\n\n\n─────────────────────────────────────────────────────────────────── Model 1 Model 2 Model 3 Model 4 Model 5\n──────────────────────────────────────────────────────── (Interce 20.09 18.71 20.10 20.09 20.09\npt) *** *** *** *** ***\n(0.54)   (0.72)   (0.83)   (0.55)   (0.50)  \n         \nwt -5.23 -4.35 -5.24 -5.37 -4.66\n*** *** *** *** ***\n(0.55)   (0.60)   (0.77)   (0.68)   (0.56)  \n         \nvs         3.15                           \n        (1.19)                          \n \nam                 -0.02                   \n \n                (1.55)                  \n \ngear                         -0.24           \n \n                        (0.68)          \n \ncarb                                 -1.33  \n                                (0.56)  \n \n──────────────────────────────────────────────────────── N 32       32       32       32       32      \n         \nR2 0.75     0.80     0.75     0.75     0.79    \n─────────────────────────────────────────────────────────────────── All continuous predictors are mean-centered and\nscaled by 1 standard deviation. *** p < 0.001;\n** p < 0.01; * p < 0.05.\nColumn names: names, Model 1, Model 2, Model 3, Model 4, Model 5\n\nO que nós observamos com essa tabela de regressão é que nenhuma variável causa um distúrbio muito grande no coeficiente de wt. Se houvesse multicolinearidade entre wt e outra variável, então o desvio padrão do coeficiente iria disparar, o valor do coeficiente mudaria bruscamente, a significância estatística diminuiria e o R2 iria aumentar muito também. Mas o que vemos, na verdade, é uma relativa estabilidade em todos esses de wt para cada modelo. É “relativa” porque os valores não são idênticos, mas tampouco são discrepantes. Rodando um VIF para cada um dos modelos de regressão múltipla, observamos que todos os valores VIFs estão abaixo de 2.\nPassemo a análise para gear.\nAnálise de gear\n\nShow code\ngear1 <- lm(mpg~gear+vs, mtcars)\ngear2 <- lm(mpg~gear+am, mtcars)\ngear3 <- lm(mpg~gear+wt, mtcars)\ngear4 <- lm(mpg~gear+carb, mtcars)\n\njtools::export_summs(gear_model, gear1, gear2, gear3, gear4, scale=TRUE)\n\n\n─────────────────────────────────────────────────────────────────── Model 1 Model 2 Model 3 Model 4 Model 5\n──────────────────────────────────────────────────────── (Interce 20.09 17.00 17.19 20.09 20.09\npt) *** *** *** *** ***\n(0.95)   (0.98)   (1.49)   (0.55)   (0.57)  \n         \ngear 2.89   2.16   0.06     -0.24    4.11 ***\n \n(0.97)   (0.76)   (1.47)   (0.68)   (0.60)  \n         \nvs         7.06 ***                        \n        (1.50)                          \n \nam                 7.14                   \n                (2.95)                  \n \nwt                         -5.37        **\n                        (0.68)          \n \ncarb                                 -4.45\n***\n                                (0.60)  \n \n──────────────────────────────────────────────────────── N 32       32       32       32       32      \n         \nR2 0.23     0.56     0.36     0.75     0.73    \n─────────────────────────────────────────────────────────────────── All continuous predictors are mean-centered and\nscaled by 1 standard deviation. *** p < 0.001;\n** p < 0.01; * p < 0.05.\nColumn names: names, Model 1, Model 2, Model 3, Model 4, Model 5\n\nRepetindo o mesmo teste, mas agora com o foco na variável gear, nós observamos um ponto interessante: a adição da variável wt faz com que gear perca totalmente a sua significância estatística. Lembrando da tabela anterior (sobre a mudança do coeficiente wt com a adição de outras variáveis), vemos que gear não influencia o valor de wt, mas que o invereso acontece. Podemos, então, avaliar se existe alguma interação entre wt e gear.\n\nShow code\ninteracao_gear_wt <- lm(mpg~gear*wt, mtcars)\n\np1 <- mtcars %>%\n  ggplot(aes(x = wt, y = mpg)) +\n  geom_point(aes(color = gear)) +\n  scale_color_gradient2(midpoint=4, low=\"#c19615\", mid=\"#1594c2\" ,high=\"#16c298\", space=\"Lab\") +\n  geom_smooth(method = \"lm\", se = FALSE, colour = \"#a3a3a3\") +\n  theme(legend.position = 'bottom')\n\np2 <- mtcars %>%\n  ggplot(aes(x = wt, y = mpg)) +\n  geom_point(aes(color = gear)) +\n  scale_color_gradient2(midpoint=4, low=\"#c19615\", mid=\"#1594c2\" ,high=\"#16c298\", space=\"Lab\") +\n  geom_smooth(method = \"lm\", se = FALSE, colour = \"#a3a3a3\") +\n  facet_wrap(~ gear, ncol=2, scales=\"free\") +\n  guides(colour = \"none\")\n\ngridExtra::grid.arrange(p1, p2, widths=c(1.5,2))\n\n\n\n\nVemos, por esse gráfico, que para cada diferente gear o efeito de wt sobre y não muda, a relação sempre é negativa.\nLevando tudo isso em consideração, podemos concluir que gear e wt não possuem nem uma relação multicolinear, nem realizam uma interação entre si. A única obseração que nos sobra é que todo o efeito que gear desempenha sobre mpg já está sendo captado por wt. Devemos, então, excluir gear do nosso modelo, a fim de ter um modelo mais parcimonioso.\nFalta agora apenas a análise de am.\nAnálise de am\n\nShow code\nam1 <- lm(mpg~am+vs, mtcars)\nam2 <- lm(mpg~am+gear, mtcars)\nam3 <- lm(mpg~am+wt, mtcars)\nam4 <- lm(mpg~am+carb, mtcars)\n\njtools::export_summs(am_model, am1, am2, am3, am4, scale=TRUE)\n\n\n─────────────────────────────────────────────────────────────────── Model 1 Model 2 Model 3 Model 4 Model 5\n──────────────────────────────────────────────────────── (Interce 17.15 14.59 17.19 20.10 16.98\npt) *** *** *** *** ***\n(1.12)   (0.93)   (1.49)   (0.83)   (0.78)  \n         \nam 7.24 *** 6.07 *** 7.14    -0.02    7.65 **\n \n(1.76)   (1.27)   (2.95)   (1.55)   (1.22)  \n         \nvs         6.93 ***                        \n        (1.26)                          \n \ngear                 0.06                    \n                (1.47)                  \n \nwt                         -5.24        \n***\n                        (0.77)          \n \ncarb                                 -3.54\n***\n                                (0.61)  \n \n──────────────────────────────────────────────────────── N 32       32       32       32       32      \n         \nR2 0.36     0.69     0.36     0.75     0.70    \n─────────────────────────────────────────────────────────────────── All continuous predictors are mean-centered and\nscaled by 1 standard deviation. *** p < 0.001;\n** p < 0.01; * p < 0.05.\nColumn names: names, Model 1, Model 2, Model 3, Model 4, Model 5\n\nO que observamos é que am, igual ao que ocorre com gear perde toda a sua significância estatística com a adição de wt. Entretanto, o inverso não é verdadeiro: am não retira o efeito de wt. Vamos então realizar a mesma análise de interação am com wt\n\nShow code\np3 <- mtcars %>%\n  ggplot(aes(x = wt, y = mpg)) +\n  geom_point(aes(color = am)) +\n  #scale_color_gradient2(midpoint=4, low=\"#c19615\", mid=\"#1594c2\" ,high=\"#16c298\", space=\"Lab\") +\n  geom_smooth(method = \"lm\", se = FALSE, colour = \"#a3a3a3\") +\n  theme(legend.position = 'bottom')\n\n\np4 <- mtcars %>%\n  ggplot(aes(x = wt, y = mpg)) +\n  geom_point(aes(color = am)) +\n  geom_smooth(method = \"lm\", se = FALSE, colour = \"#a3a3a3\") +\n  facet_wrap(~ am, ncol=2, scales=\"free\") +\n  guides(colour = \"none\")\n\n\ngridExtra::grid.arrange(p3, p4, widths=c(1.5,2))\n\n\n\n\nNovamente, o que observamos é que am não influencia o resultado de wt. Não há nem multicolineridade, nem interação. A melhor solução, portanto, é retirar am do modelo.\nAnálise de vs e carb\nAté aqui vimos que cyl e hp tinham multicolinearidade com as demais variáveis porque possuíam VIFs extremamente altos. Então excluímos eles do modelo. Vimos também que os efeitos de gear e de am sobre mpg eram absorvidos por wt. Então também excluímos eles, a fim de deixar o modelo mais parcimonioso.\nDas 7 variáveis inciais, portanto, sobraram 3: wt, vs e carb. Já análisamos wt. Falta apenas vs e carb.\nSe começarmos rodando uma regressão com as três variáveis faltantes, observamos que, combinadas com wt, tanto carb quanto vs perdem sua significância estatística.\n\nShow code\na <- lm(mpg~wt + vs + carb, mtcars)\nsummary(a)\n\n\n\nCall:\nlm(formula = mpg ~ wt + vs + carb, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.0980 -2.3545  0.1704  1.2852  5.6616 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  34.3808     2.5190  13.648 6.75e-14 ***\nwt           -4.3036     0.6118  -7.035 1.19e-07 ***\nvs            2.3489     1.3062   1.798   0.0829 .  \ncarb         -0.5234     0.3751  -1.395   0.1739    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.735 on 28 degrees of freedom\nMultiple R-squared:  0.8139,    Adjusted R-squared:  0.794 \nF-statistic: 40.83 on 3 and 28 DF,  p-value: 2.349e-10\n\nIsso não indica multicolinearidade porque os VIFs das variáveis são muito baixos.\n\nShow code\ncar::vif(a)\n\n\n      wt       vs     carb \n1.484433 1.795593 1.520735 \n\nAssim, temos que fazer uma análise mais minuciosa sobre o comportamento dos coeficientes ao adicionarmos as variáveis uma a uma.\n\nShow code\nwt1 <- lm(mpg~wt+vs, mtcars)\nwt4 <- lm(mpg~wt+carb, mtcars)\nwt5 <- lm(mpg~vs+carb, mtcars)\n\njtools::export_summs(wt_model, vs_model, carb_model, wt1, wt4, wt5, a, scale=TRUE)\n\n\n─────────────────────────────────────────────────────────────────── Model 1 Model 2 Model 3 Model 4 Model 5\n──────────────────────────────────────────────────────── (Interce 20.09 16.62 20.09 18.71 20.09\npt) *** *** *** *** ***\n(0.54)   (1.08)   (0.90)   (0.72)   (0.50)  \n         \nwt -5.23                 -4.35 -4.66\n*** *** ***\n(0.55)                   (0.60)   (0.56)  \n     \nvs         7.94 ***         3.15 *          \n        (1.63)           (1.19)          \n   \ncarb                 -3.32         -1.33    \n                (0.92)           (0.56)  \n   \n──────────────────────────────────────────────────────── N 32       32       32       32       32      \n         \nR2 0.75     0.44     0.30     0.80     0.79    \n─────────────────────────────────────────────────────────────────── All continuous predictors are mean-centered and\nscaled by 1 standard deviation.  p < 0.001;\n** p < 0.01; * p < 0.05.\nColumn names: names, Model 1, Model 2, Model 3, Model 4, Model 5, Model 6, Model 7\n6/8 columns shown.\n\nO que observamos é que as variáveis vs e carb, sem a presença de wt possuem uma significância estatística alta, a 99,999% (Modelos 1, 2 e 3). Combinadas individualmente com wt, elas ainda mantém sua significância estatística (modelos 4 e 5), mas a 95% de certeza. Em um modelo sem wt, mas com ambas as variáveis vse carb, o que observamos é que carb perde sua significância, enquanto vs se mantém significante a 99% (modelo 6).\nPara continuar a análise, iremos observar o que acontece quando as variáveis interagem entre si.\n\nShow code\nwt6 <- lm(mpg~wt+vs+wt*vs, mtcars)\nwt7 <- lm(mpg~wt+carb+wt*carb, mtcars)\nwt8 <- lm(mpg~carb+vs+carb*vs, mtcars)\n\njtools::export_summs(wt_model, wt6, wt7, wt8, scale=TRUE)\n\n\n────────────────────────────────────────────────────────────────── Model 1 Model 2 Model 3 Model 4\n───────────────────────────────────────────────────── (Intercept 20.09 *** 18.27 *** 19.34 *** 17.06 ***\n)\n(0.54)    (0.69)    (0.53)    (1.19)   \nwt -5.23 *** -3.43 *** -4.81 ***        \n(0.55)    (0.68)    (0.52)           \nvs         2.41            5.37   \n        (1.15)            (2.07)   \nwt:vs         -2.85                   \n        (1.19)                   \ncarb                 -1.16    -0.90    \n                (0.52)    (1.13)   \nwt:carb                 1.81           \n                (0.68)           \ncarb:vs                         -2.45    \n                        (2.21)   \n───────────────────────────────────────────────────── N 32        32        32        32       \nR2 0.75     0.83     0.83     0.51    \n────────────────────────────────────────────────────────────────── All continuous predictors are mean-centered and\nscaled by 1 standard deviation. ** p < 0.001; **\np < 0.01; * p < 0.05.\nColumn names: names, Model 1, Model 2, Model 3, Model 4\n\nO que observamos com as interações é que, quando vs e carb interagem individualmente com wt, todos os coeficientes se mantém estatisticamente relevantes (modelos 2 e 3). Entretanto, quando realizamos uma interação apenas entre essas duas variáveis, carb perde sua significância (modelo 4). Se carb perde a significância quando interage com vs, então vou optar por manter no modelo apenas vs, a fim de garantir um modelo mais parcimonioso.\nConclusão parcial\nCom todas as alterações no modelo, ficamos apenas com duas variáveis no final e uma interação:\n\\[\n\\operatorname{adjusted \\ model:\\ mpg} = \\beta_{0} + \\beta_{1}(\\operatorname{wt}) + \\beta_{2}(\\operatorname{vs}) + \\beta_{3}(\\operatorname{wt*vs}) + \\epsilon\n\\]\nCalculando os coeficientes, ficamos com o seguinte modelo: \\[\n\\operatorname{adjusted \\ model:\\ mpg} = 29.53 - 3.50(\\operatorname{wt}) + 11.77(\\operatorname{vs}) - 2.90(\\operatorname{wt*vs}) + \\epsilon\n\\]\nDiagnóstico do modelo\nPor fim, resta apenas avaliar se o modelo ficou bom ou não.\n\nShow code\nplot(adjusted)\n\n\n\n\nA começar pelo gráfico de resíduos, há duas observações improtantes. Primeiro, os resíduos não ficaram iguais a zero. Isso é bom, uma vez que resíduo 0 significaria um sobreajuste do modelo. Seria tão artificial que seria um modelo ruim. A segunda obseravção importante é que a forma como os resíduos se distribuem não possui nenhum padrão. A distribuição aleatória dos resíduos indica um modelo saudável.\nAlém disso, analisando o gráfico que nos indica a distância de Cook e o efeito alavanca de outliers, o que observamos é que não existem leverage outliers. Os pontos estão bem distantes da região de cook.\nConclusão e discussão\nNo fim, portanto, podemos concluir que a melhor explicação que podemos dar para a eficiência dos carros envolve o peso (wt) e o tipo de motor (vs). Essas variáveis interagem entre si, porque cada tipo de motor possui um peso diferente.\nComo uma de nossas variáveis é binária, o intercepto se torna uma casela de referência. Essa casela indica a eficiência do carro cujo motor é V-shaped e não pesa nada (algo que só é possível em condições de laboratório, quando anulamos a gravidade). Este motor (v-shaped, sem peso) roda 29.53 milhas por galão de combustível. Se mudarmos o motor para um motor straight, então a eficiência aumenta em 11 milhas por galão, em média, ficando, aproximadamente, 41 mpg. Mas isso só se aplica também nas condições ideais sem gravidade, em que o peso permanece 0.\nQuando começamos a considerar a variável de peso, então a interpretação do modelo se modifica ligeiramente, por causa da interação. No caso do motor V-shaped, para cada 1000 libras que o carro ganha, a sua eficiência diminui em 3.5 milhas por galão. Para o motor straight, a cada 1000 libras que o carro ganha, então a eficiência do motor diminui em média 5.9 milhas por galão.\n\n\n\n",
    "preview": "posts/2021-02-05-tcc/tcc_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-02-05T15:56:30-03:00",
    "input_file": {}
  }
]
